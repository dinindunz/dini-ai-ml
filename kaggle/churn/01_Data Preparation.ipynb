{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import lit\n",
    "import shutil\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this has been added for scenarios where you might\n",
    "# wish to alter some of the churn label prediction\n",
    "# logic but do not wish to rerun the whole notebook\n",
    "skip_reload = False\n",
    "\n",
    "# please use a personalized database name here if you wish to avoid interfering with other users who might be running this accelerator in the same workspace\n",
    "database_name = 'kkbox_churn'\n",
    "data_dir = f\"{os.getenv('HOME')}/databricks/kkbox_churn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()  # Properly stop Spark\n",
    "del spark     # Delete the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"ChurnCluster\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.2.1\") \\\n",
    "    .config(\"spark.executor.memory\", \"56g\") \\\n",
    "    .config(\"spark.driver.memory\", \"56g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "os.environ[\"SPARK_APP_NAME\"] = spark.conf.get(\"spark.app.name\")\n",
    "os.environ[\"SPARK_MASTER\"] = spark.conf.get(\"spark.master\")\n",
    "\n",
    "print(\"Spark Version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if skip_reload:\n",
    "  # create database to house SQL tables\n",
    "  _ = spark.sql(f'CREATE DATABASE IF NOT EXISTS {database_name}')\n",
    "  _ = spark.sql(f'USE {database_name}')\n",
    "else:\n",
    "  # delete the old database if needed\n",
    "  _ = spark.sql(f'DROP DATABASE IF EXISTS {database_name} CASCADE')\n",
    "  _ = spark.sql(f'CREATE DATABASE {database_name}')\n",
    "  _ = spark.sql(f'USE {database_name}')\n",
    "\n",
    "  # drop any old delta lake files that might have been created\n",
    "  folder_path = f'{data_dir}/silver/members'\n",
    "  if os.path.exists(folder_path):\n",
    "      shutil.rmtree(folder_path)\n",
    "    \n",
    "  # members dataset schema\n",
    "  member_schema = StructType([\n",
    "    StructField('msno', StringType()),\n",
    "    StructField('city', IntegerType()),\n",
    "    StructField('bd', IntegerType()),\n",
    "    StructField('gender', StringType()),\n",
    "    StructField('registered_via', IntegerType()),\n",
    "    StructField('registration_init_time', DateType())\n",
    "    ])\n",
    "\n",
    "  # read data from csv\n",
    "  members = (\n",
    "    spark\n",
    "      .read\n",
    "      .csv(\n",
    "        f'{data_dir}/members/members_v3.csv',\n",
    "        schema=member_schema,\n",
    "        header=True,\n",
    "        dateFormat='yyyyMMdd'\n",
    "        )\n",
    "      )\n",
    "\n",
    "  # persist in delta lake format\n",
    "  (\n",
    "    members\n",
    "      .write\n",
    "      .format('delta')\n",
    "      .mode('overwrite')\n",
    "      .save(f'{data_dir}/silver/members')\n",
    "    )\n",
    "\n",
    "    # create table object to make delta lake queryable\n",
    "  _ = spark.sql('''\n",
    "      CREATE TABLE members \n",
    "      USING DELTA \n",
    "      LOCATION '/home/dinindu/databricks/kkbox_churn/silver/members'\n",
    "      ''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(members.show())\n",
    "result = spark.sql(\"SELECT * FROM kkbox_churn.members LIMIT 10\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_reload:\n",
    "\n",
    "  # drop any old delta lake files that might have been created\n",
    "  folder_path = f'{data_dir}/silver/transactions'\n",
    "  if os.path.exists(folder_path):\n",
    "      shutil.rmtree(folder_path)\n",
    "\n",
    "  # transaction dataset schema\n",
    "  transaction_schema = StructType([\n",
    "    StructField('msno', StringType()),\n",
    "    StructField('payment_method_id', IntegerType()),\n",
    "    StructField('payment_plan_days', IntegerType()),\n",
    "    StructField('plan_list_price', IntegerType()),\n",
    "    StructField('actual_amount_paid', IntegerType()),\n",
    "    StructField('is_auto_renew', IntegerType()),\n",
    "    StructField('transaction_date', DateType()),\n",
    "    StructField('membership_expire_date', DateType()),\n",
    "    StructField('is_cancel', IntegerType())  \n",
    "    ])\n",
    "\n",
    "  # read data from csv\n",
    "  transactions = (\n",
    "    spark\n",
    "      .read\n",
    "      .csv(\n",
    "        f'{data_dir}/transactions/transactions.csv',\n",
    "        schema=transaction_schema,\n",
    "        header=True,\n",
    "        dateFormat='yyyyMMdd'\n",
    "        )\n",
    "      )\n",
    "\n",
    "  # persist in delta lake format\n",
    "  ( transactions\n",
    "      .write\n",
    "      .format('delta')\n",
    "      .partitionBy('transaction_date')\n",
    "      .mode('overwrite')\n",
    "      .save(f'{data_dir}/silver/transactions')\n",
    "    )\n",
    "\n",
    "    # create table object to make delta lake queryable\n",
    "  _ = spark.sql('''\n",
    "      CREATE TABLE transactions\n",
    "      USING DELTA \n",
    "      LOCATION '/home/dinindu/databricks/kkbox_churn/silver/transactions'\n",
    "      ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(transactions.show())\n",
    "result = spark.sql(\"SELECT * FROM kkbox_churn.transactions LIMIT 10\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_reload:\n",
    "  # drop any old delta lake files that might have been created\n",
    "  folder_path = f'{data_dir}/silver/user_logs'\n",
    "  if os.path.exists(folder_path):\n",
    "      shutil.rmtree(folder_path)\n",
    "\n",
    "  # transaction dataset schema\n",
    "  user_logs_schema = StructType([ \n",
    "    StructField('msno', StringType()),\n",
    "    StructField('date', DateType()),\n",
    "    StructField('num_25', IntegerType()),\n",
    "    StructField('num_50', IntegerType()),\n",
    "    StructField('num_75', IntegerType()),\n",
    "    StructField('num_985', IntegerType()),\n",
    "    StructField('num_100', IntegerType()),\n",
    "    StructField('num_uniq', IntegerType()),\n",
    "    StructField('total_secs', FloatType())  \n",
    "    ])\n",
    "\n",
    "  # read data from csv\n",
    "  user_logs = (\n",
    "    spark\n",
    "      .read\n",
    "      .csv(\n",
    "        f'{data_dir}/user_logs/user_logs.csv',\n",
    "        schema=user_logs_schema,\n",
    "        header=True,\n",
    "        dateFormat='yyyyMMdd'\n",
    "        )\n",
    "      )\n",
    "\n",
    "  # persist in delta lake format\n",
    "  ( user_logs\n",
    "      .write\n",
    "      .format('delta')\n",
    "      .partitionBy('date')\n",
    "      .mode('overwrite')\n",
    "      .save(f'{data_dir}/silver/user_logs')\n",
    "    )\n",
    "\n",
    "  # create table object to make delta lake queryable\n",
    "  _ = spark.sql('''\n",
    "    CREATE TABLE IF NOT EXISTS user_logs\n",
    "    USING DELTA \n",
    "    LOCATION '/home/dinindu/databricks/kkbox_churn/silver/user_logs'\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete training labels if exists before create\n",
    "%%sh -e\n",
    "rm -rf \"$data_dir/silver/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete training labels if exists before create\n",
    "_ = spark.sql('DROP TABLE IF EXISTS train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e3885f44e867279da8f2bccd336017454ceececd5e32b0053ce7bb68babc0327\n",
      "ecc0bdeac27a5c6e65c348ec477db82f6e5fc6afdad31600e8b1ef4381e2f67f\n"
     ]
    }
   ],
   "source": [
    "%%sh -e\n",
    "\n",
    "# docker network create spark-net\n",
    "# docker build -t dini-spark:3.5.4 .\n",
    "\n",
    "docker run -d --rm --network spark-net --name spark-master \\\n",
    "    -p 8080:8080 -p 7077:7077 -p 4040:4040 \\\n",
    "    bitnami/spark spark-class org.apache.spark.deploy.master.Master \\\n",
    "    # --packages io.delta:delta-core_2.13:2.4.0\n",
    "\n",
    "docker run -d --rm --network spark-net --name spark-worker \\\n",
    "    --env SPARK_MODE=worker \\\n",
    "    --env SPARK_MASTER_URL=spark://spark-master:7077 \\\n",
    "    bitnami/spark spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077 \\\n",
    "    # --packages io.delta:delta-core_2.13:2.4.0\n",
    "\n",
    "# docker run -d --rm --network spark-net --name spark-master \\\n",
    "#     -p 8080:8080 -p 7077:7077 -p 4040:4040 \\\n",
    "#     apache/spark spark-class org.apache.spark.deploy.master.Master \\\n",
    "#     --packages io.delta:delta-core_2.12:2.3.0\n",
    "\n",
    "# docker run -d --rm --network spark-net --name spark-worker \\\n",
    "#     --env SPARK_MODE=worker \\\n",
    "#     --env SPARK_MASTER_URL=spark://spark-master:7077 \\\n",
    "#     apache/spark spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077 \\\n",
    "#     --packages io.delta:delta-core_2.12:2.3.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;6mspark \u001b[38;5;5m08:08:31.81 \u001b[0m\u001b[38;5;2mINFO \u001b[0m ==> \n",
      "\u001b[38;5;6mspark \u001b[38;5;5m08:08:31.81 \u001b[0m\u001b[38;5;2mINFO \u001b[0m ==> \u001b[1mWelcome to the Bitnami spark container\u001b[0m\n",
      "\u001b[38;5;6mspark \u001b[38;5;5m08:08:31.81 \u001b[0m\u001b[38;5;2mINFO \u001b[0m ==> Subscribe to project updates by watching \u001b[1mhttps://github.com/bitnami/containers\u001b[0m\n",
      "\u001b[38;5;6mspark \u001b[38;5;5m08:08:31.82 \u001b[0m\u001b[38;5;2mINFO \u001b[0m ==> Did you know there are enterprise versions of the Bitnami catalog? For enhanced secure software supply chain features, unlimited pulls from Docker, LTS support, or application customization, see Bitnami Premium or Tanzu Application Catalog. See https://www.arrow.com/globalecs/na/vendors/bitnami/ for more information.\n",
      "\u001b[38;5;6mspark \u001b[38;5;5m08:08:31.82 \u001b[0m\u001b[38;5;2mINFO \u001b[0m ==> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ":: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache\n",
      "The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars\n",
      "io.delta#delta-core_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f539d592-96ef-4162-8b73-d9ea1b728e95;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.13;2.4.0 in central\n",
      "\tfound io.delta#delta-storage;2.4.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-core_2.13/2.4.0/delta-core_2.13-2.4.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-core_2.13;2.4.0!delta-core_2.13.jar (1559ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-storage/2.4.0/delta-storage-2.4.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-storage;2.4.0!delta-storage.jar (473ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.9.3/antlr4-runtime-4.9.3.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.9.3!antlr4-runtime.jar (411ms)\n",
      ":: resolution report :: resolve 4632ms :: artifacts dl 2450ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.13;2.4.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.4.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f539d592-96ef-4162-8b73-d9ea1b728e95\n",
      "\tconfs: [default]\n",
      "\t3 artifacts copied, 0 already retrieved (4596kB/10ms)\n",
      "25/02/15 08:08:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark context Web UI available at http://949af931d6cc:4040\n",
      "Spark context available as 'sc' (master = local[*], app id = local-1739606923278).\n",
      "Spark session available as 'spark'.\n",
      "java.util.ServiceConfigurationError: org.apache.spark.sql.sources.DataSourceRegister: org.apache.spark.sql.delta.sources.DeltaDataSource Unable to get public no-arg constructor\n",
      "  at java.base/java.util.ServiceLoader.fail(Unknown Source)\n",
      "  at java.base/java.util.ServiceLoader.getConstructor(Unknown Source)\n",
      "  at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNextService(Unknown Source)\n",
      "  at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNext(Unknown Source)\n",
      "  at java.base/java.util.ServiceLoader$2.hasNext(Unknown Source)\n",
      "  at java.base/java.util.ServiceLoader$3.hasNext(Unknown Source)\n",
      "  at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "  at scala.collection.TraversableLike.filterImpl(TraversableLike.scala:303)\n",
      "  at scala.collection.TraversableLike.filterImpl$(TraversableLike.scala:297)\n",
      "  at scala.collection.AbstractTraversable.filterImpl(Traversable.scala:108)\n",
      "  at scala.collection.TraversableLike.filter(TraversableLike.scala:395)\n",
      "  at scala.collection.TraversableLike.filter$(TraversableLike.scala:395)\n",
      "  at scala.collection.AbstractTraversable.filter(Traversable.scala:108)\n",
      "  at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:629)\n",
      "  at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n",
      "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n",
      "  at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n",
      "  at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:444)\n",
      "  ... 60 elided\n",
      "Caused by: java.lang.NoClassDefFoundError: scala/collection/IterableOnce\n",
      "  at java.base/java.lang.Class.getDeclaredConstructors0(Native Method)\n",
      "  at java.base/java.lang.Class.privateGetDeclaredConstructors(Unknown Source)\n",
      "  at java.base/java.lang.Class.getConstructor0(Unknown Source)\n",
      "  at java.base/java.lang.Class.getConstructor(Unknown Source)\n",
      "  at java.base/java.util.ServiceLoader$1.run(Unknown Source)\n",
      "  at java.base/java.util.ServiceLoader$1.run(Unknown Source)\n",
      "  at java.base/java.security.AccessController.doPrivileged(Unknown Source)\n",
      "  ... 83 more\n",
      "Caused by: java.lang.ClassNotFoundException: scala.collection.IterableOnce\n",
      "  at java.base/java.net.URLClassLoader.findClass(Unknown Source)\n",
      "  at java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n",
      "  at java.base/java.lang.ClassLoader.loadClass(Unknown Source)\n",
      "  ... 90 more\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.4\n",
      "      /_/\n",
      "         \n",
      "Using Scala version 2.12.18 (OpenJDK 64-Bit Server VM, Java 17.0.14)\n",
      "Type in expressions to have them evaluated.\n",
      "Type :help for more information.\n",
      "\n",
      "scala> :quit\n"
     ]
    }
   ],
   "source": [
    "%%sh -e\n",
    "\n",
    "docker run --rm --network spark-net  \\\n",
    "    -v \"/home/dinindu/databricks/kkbox_churn:/opt/spark/work/kkbox_churn\" \\\n",
    "    -v \"$PWD:/opt/bitnami/spark/work\" \\\n",
    "    bitnami/spark  spark-shell --master local[*] \\\n",
    "    --packages io.delta:delta-core_2.12:1.2.1 \\\n",
    "    -i /opt/bitnami/spark/work/scripts/generate_training_labels.scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete training labels if exists before create\n",
    "%%sh -e\n",
    "# rm -rf \"$data_dir/silver/train\"\n",
    "scala --version\n",
    "# scala \"scripts/generate_traning_labels.scala\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "db-churn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
