{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import lit\n",
    "import shutil\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this has been added for scenarios where you might\n",
    "# wish to alter some of the churn label prediction\n",
    "# logic but do not wish to rerun the whole notebook\n",
    "skip_reload = False\n",
    "\n",
    "# please use a personalized database name here if you wish to avoid interfering with other users who might be running this accelerator in the same workspace\n",
    "database_name = 'kkbox_churn'\n",
    "data_dir = f\"{os.getenv('HOME')}/databricks/kkbox_churn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()  # Properly stop Spark\n",
    "del spark     # Delete the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"ChurnCluster\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.2.1\") \\\n",
    "    .config(\"spark.executor.memory\", \"56g\") \\\n",
    "    .config(\"spark.driver.memory\", \"56g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "os.environ[\"SPARK_APP_NAME\"] = spark.conf.get(\"spark.app.name\")\n",
    "os.environ[\"SPARK_MASTER\"] = spark.conf.get(\"spark.master\")\n",
    "\n",
    "print(\"Spark Version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if skip_reload:\n",
    "  # create database to house SQL tables\n",
    "  _ = spark.sql(f'CREATE DATABASE IF NOT EXISTS {database_name}')\n",
    "  _ = spark.sql(f'USE {database_name}')\n",
    "else:\n",
    "  # delete the old database if needed\n",
    "  _ = spark.sql(f'DROP DATABASE IF EXISTS {database_name} CASCADE')\n",
    "  _ = spark.sql(f'CREATE DATABASE {database_name}')\n",
    "  _ = spark.sql(f'USE {database_name}')\n",
    "\n",
    "  # drop any old delta lake files that might have been created\n",
    "  folder_path = f'{data_dir}/silver/members'\n",
    "  if os.path.exists(folder_path):\n",
    "      shutil.rmtree(folder_path)\n",
    "    \n",
    "  # members dataset schema\n",
    "  member_schema = StructType([\n",
    "    StructField('msno', StringType()),\n",
    "    StructField('city', IntegerType()),\n",
    "    StructField('bd', IntegerType()),\n",
    "    StructField('gender', StringType()),\n",
    "    StructField('registered_via', IntegerType()),\n",
    "    StructField('registration_init_time', DateType())\n",
    "    ])\n",
    "\n",
    "  # read data from csv\n",
    "  members = (\n",
    "    spark\n",
    "      .read\n",
    "      .csv(\n",
    "        f'{data_dir}/members/members_v3.csv',\n",
    "        schema=member_schema,\n",
    "        header=True,\n",
    "        dateFormat='yyyyMMdd'\n",
    "        )\n",
    "      )\n",
    "\n",
    "  # persist in delta lake format\n",
    "  (\n",
    "    members\n",
    "      .write\n",
    "      .format('delta')\n",
    "      .mode('overwrite')\n",
    "      .save(f'{data_dir}/silver/members')\n",
    "    )\n",
    "\n",
    "    # create table object to make delta lake queryable\n",
    "  _ = spark.sql('''\n",
    "      CREATE TABLE members \n",
    "      USING DELTA \n",
    "      LOCATION '/home/dinindu/databricks/kkbox_churn/silver/members'\n",
    "      ''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(members.show())\n",
    "result = spark.sql(\"SELECT * FROM kkbox_churn.members LIMIT 10\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_reload:\n",
    "\n",
    "  # drop any old delta lake files that might have been created\n",
    "  folder_path = f'{data_dir}/silver/transactions'\n",
    "  if os.path.exists(folder_path):\n",
    "      shutil.rmtree(folder_path)\n",
    "\n",
    "  # transaction dataset schema\n",
    "  transaction_schema = StructType([\n",
    "    StructField('msno', StringType()),\n",
    "    StructField('payment_method_id', IntegerType()),\n",
    "    StructField('payment_plan_days', IntegerType()),\n",
    "    StructField('plan_list_price', IntegerType()),\n",
    "    StructField('actual_amount_paid', IntegerType()),\n",
    "    StructField('is_auto_renew', IntegerType()),\n",
    "    StructField('transaction_date', DateType()),\n",
    "    StructField('membership_expire_date', DateType()),\n",
    "    StructField('is_cancel', IntegerType())  \n",
    "    ])\n",
    "\n",
    "  # read data from csv\n",
    "  transactions = (\n",
    "    spark\n",
    "      .read\n",
    "      .csv(\n",
    "        f'{data_dir}/transactions/transactions.csv',\n",
    "        schema=transaction_schema,\n",
    "        header=True,\n",
    "        dateFormat='yyyyMMdd'\n",
    "        )\n",
    "      )\n",
    "\n",
    "  # persist in delta lake format\n",
    "  ( transactions\n",
    "      .write\n",
    "      .format('delta')\n",
    "      .partitionBy('transaction_date')\n",
    "      .mode('overwrite')\n",
    "      .save(f'{data_dir}/silver/transactions')\n",
    "    )\n",
    "\n",
    "    # create table object to make delta lake queryable\n",
    "  _ = spark.sql('''\n",
    "      CREATE TABLE transactions\n",
    "      USING DELTA \n",
    "      LOCATION '/home/dinindu/databricks/kkbox_churn/silver/transactions'\n",
    "      ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(transactions.show())\n",
    "result = spark.sql(\"SELECT * FROM kkbox_churn.transactions LIMIT 10\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_reload:\n",
    "  # drop any old delta lake files that might have been created\n",
    "  folder_path = f'{data_dir}/silver/user_logs'\n",
    "  if os.path.exists(folder_path):\n",
    "      shutil.rmtree(folder_path)\n",
    "\n",
    "  # transaction dataset schema\n",
    "  user_logs_schema = StructType([ \n",
    "    StructField('msno', StringType()),\n",
    "    StructField('date', DateType()),\n",
    "    StructField('num_25', IntegerType()),\n",
    "    StructField('num_50', IntegerType()),\n",
    "    StructField('num_75', IntegerType()),\n",
    "    StructField('num_985', IntegerType()),\n",
    "    StructField('num_100', IntegerType()),\n",
    "    StructField('num_uniq', IntegerType()),\n",
    "    StructField('total_secs', FloatType())  \n",
    "    ])\n",
    "\n",
    "  # read data from csv\n",
    "  user_logs = (\n",
    "    spark\n",
    "      .read\n",
    "      .csv(\n",
    "        f'{data_dir}/user_logs/user_logs.csv',\n",
    "        schema=user_logs_schema,\n",
    "        header=True,\n",
    "        dateFormat='yyyyMMdd'\n",
    "        )\n",
    "      )\n",
    "\n",
    "  # persist in delta lake format\n",
    "  ( user_logs\n",
    "      .write\n",
    "      .format('delta')\n",
    "      .partitionBy('date')\n",
    "      .mode('overwrite')\n",
    "      .save(f'{data_dir}/silver/user_logs')\n",
    "    )\n",
    "\n",
    "  # create table object to make delta lake queryable\n",
    "  _ = spark.sql('''\n",
    "    CREATE TABLE IF NOT EXISTS user_logs\n",
    "    USING DELTA \n",
    "    LOCATION '/home/dinindu/databricks/kkbox_churn/silver/user_logs'\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete training labels if exists before create\n",
    "_ = spark.sql('DROP TABLE IF EXISTS train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh -e\n",
    "\n",
    "docker network create spark-net\n",
    "\n",
    "docker run -d --rm --network spark-net --name spark-master \\\n",
    "    -p 8080:8080 -p 7077:7077 -p 4040:4040 \\\n",
    "    bitnami/spark spark-class org.apache.spark.deploy.master.Master\n",
    "\n",
    "docker run -d --rm --network spark-net --name spark-worker \\\n",
    "    --env SPARK_MODE=worker \\\n",
    "    --env SPARK_MASTER_URL=spark://spark-master:7077 \\\n",
    "    bitnami/spark spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh -e\n",
    "\n",
    "kkbox_churn_dir=\"/home/dinindu/databricks/kkbox_churn\"\n",
    "sudo chmod 777 $kkbox_churn_dir\n",
    "sudo rm -rf $kkbox_churn_dir/silver/train\n",
    "\n",
    "docker run --rm --network host  \\\n",
    "    -v \"$kkbox_churn_dir:/opt/spark/work/kkbox_churn\" \\\n",
    "    -v \"$PWD:/opt/bitnami/spark/work\" \\\n",
    "    bitnami/spark:3.4.1 spark-shell --master local[*] \\\n",
    "    --executor-memory 48G \\\n",
    "    --driver-memory 16G \\\n",
    "    --packages io.delta:delta-core_2.12:2.4.0 \\\n",
    "    --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\\n",
    "    --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog \\\n",
    "    -i /opt/bitnami/spark/work/scripts/generate_training_labels.scala\n",
    "\n",
    "sudo chown -R dinindu:dinindu $kkbox_churn_dir/silver/train\n",
    "sudo chmod -R 777 $kkbox_churn_dir/silver/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete training labels if exists before create\n",
    "%%sh -e\n",
    "# rm -rf \"$data_dir/silver/train\"\n",
    "scala --version\n",
    "# scala \"scripts/generate_traning_labels.scala\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "db-churn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
